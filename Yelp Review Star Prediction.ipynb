{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, RNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = 'data'\n",
    "SEED = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    pickle_name = os.path.join(DATA_ROOT, file_name + '.pkl')\n",
    "    time_start = datetime.now()\n",
    "    if os.path.isfile(pickle_name):\n",
    "        print('loading from pickle...')\n",
    "        review_data = pd.read_pickle(pickle_name)\n",
    "    else:\n",
    "        print('loading from csv...')\n",
    "        review_data = pd.read_csv(os.path.join(DATA_ROOT, file_name))\n",
    "        review_data.to_pickle(pickle_name)\n",
    "    print('Loaded in ' + str(datetime.now() - time_start) + ' seconds')\n",
    "    return review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(vocab_size, train_text=None):\n",
    "    tokenizer_file_name = os.path.join(DATA_ROOT, 'tokenizers', 'tokenizer_' + str(vocab_size) + '.pkl')\n",
    "    time_start = datetime.now()\n",
    "    if os.path.isfile(tokenizer_file_name):\n",
    "        print('Loading tokenizer...')\n",
    "        with open(tokenizer_file_name, 'rb') as file:\n",
    "            tokenizer = pickle.load(file)\n",
    "    else:\n",
    "        print('Training tokenizer...')\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        tokenizer.fit_on_texts(train_text)\n",
    "        \n",
    "        with open(tokenizer_file_name, 'wb') as file:\n",
    "            pickle.dump(tokenizer, file)\n",
    "        \n",
    "    print('Got tokenizer for vocab size: ' + str(vocab_size) + ' in ' + str(datetime.now() - time_start))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(vocab_size=100, review_length=250, num_reviews=None):\n",
    "    print('Retrieving/Preparing data for: vocab size = ' + str(vocab_size) + ' review_length = ' + str(review_length) + ' num_reviews = ' + str(num_reviews))\n",
    "    file_name = os.path.join(DATA_ROOT, 'processed_data', str(vocab_size) + '_' + str(review_length) + '_' + str(num_reviews) + '.pkl')\n",
    "    if not os.path.isfile(file_name):\n",
    "        review_data = load_data('yelp_review.csv')\n",
    "        review_data.drop(['review_id', 'user_id', 'business_id', 'date', 'useful', 'funny', 'cool'], axis=1, inplace=True)\n",
    "        \n",
    "        x = review_data['text'].as_matrix()\n",
    "        y = pd.get_dummies(review_data['stars']).as_matrix()\n",
    "        \n",
    "        # We want our tokenizer on all of the data\n",
    "        tokenizer = get_tokenizer(vocab_size)\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.3, train_size=0.7, random_state=SEED)\n",
    "        \n",
    "        # The data is truncated after the split so we get a stratified sample\n",
    "        \n",
    "        if num_reviews:\n",
    "            print('Truncating data...')\n",
    "            x_train = x_train[0:num_reviews]\n",
    "            x_test = x_test[0:num_reviews]\n",
    "            y_train = y_train[0:num_reviews]\n",
    "            y_test = y_test[0:num_reviews]\n",
    "        \n",
    "        x_train = tokenizer.texts_to_sequences(x_train)\n",
    "        x_train = pad_sequences(x_train, maxlen=review_length)\n",
    "\n",
    "        # Fit our testing data\n",
    "        x_test = tokenizer.texts_to_sequences(x_test)\n",
    "        x_test = pad_sequences(x_test, maxlen=review_length)\n",
    "        \n",
    "        with open(file_name, 'wb') as file:\n",
    "            pickle.dump([x_train, x_test, y_train, y_test], file)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(vocab_size=100, review_length=250, num_reviews=None):\n",
    "    prep_data(vocab_size=vocab_size, review_length=review_length, num_reviews=num_reviews)\n",
    "    \n",
    "    file_name = os.path.join(DATA_ROOT, 'processed_data', str(vocab_size) + '_' + str(review_length) + '_' + str(num_reviews) + '.pkl')\n",
    "    \n",
    "    with open(file_name, 'rb') as file:\n",
    "            x_train, x_test, y_train, y_test = pickle.load(file)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving/Preparing data for: vocab size = 50 review_length = 500 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 50 review_length = 250 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 50 review_length = 100 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 100 review_length = 100 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 100 review_length = 250 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 100 review_length = 500 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 150 review_length = 250 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 150 review_length = 100 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 150 review_length = 500 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 200 review_length = 100 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 200 review_length = 250 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 200 review_length = 500 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 250 review_length = 100 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 250 review_length = 250 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 250 review_length = 500 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 500 review_length = 100 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 500 review_length = 250 num_reviews = 25000\n",
      "Retrieving/Preparing data for: vocab size = 500 review_length = 500 num_reviews = 25000\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "review_lengths = [100, 250, 500]\n",
    "vocab_sizes = [50, 100, 150, 200, 250, 500]\n",
    "\n",
    "_ = Parallel(n_jobs=4)(\n",
    "    delayed(prep_data)(\n",
    "        vocab_size=vocab_size, review_length=review_length, num_reviews=25000)\n",
    "    for vocab_size in vocab_sizes\n",
    "    for review_length in review_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Explain rational behind this\n",
    "'''\n",
    "def mean_star_diff(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.argmax(y_true) - K.argmax(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_lstm_model(embedding_vector_length=32, dropout_rate=0.2, vocab_size=500, review_length=250):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_vector_length, input_length=review_length))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', mean_star_diff])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving/Preparing data for: vocab size = 50 review_length = 150 num_reviews = 25000\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 1.3244 - acc: 0.4557 - mean_star_diff: 0.7322\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 76s 3ms/step - loss: 1.2253 - acc: 0.4960 - mean_star_diff: 0.3984 1s - loss: 1.2251 - acc: 0.4963 - me\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 77s 3ms/step - loss: 1.2202 - acc: 0.5015 - mean_star_diff: 0.3677\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 1.2111 - acc: 0.5008 - mean_star_diff: 0.3814\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 102s 4ms/step - loss: 1.2034 - acc: 0.5067 - mean_star_diff: 0.3370\n",
      "Retrieving/Preparing data for: vocab size = 50 review_length = 250 num_reviews = 25000\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 168s 7ms/step - loss: 1.3260 - acc: 0.4588 - mean_star_diff: 0.7133\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 150s 6ms/step - loss: 1.2736 - acc: 0.4747 - mean_star_diff: 0.5981\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 143s 6ms/step - loss: 1.2247 - acc: 0.4982 - mean_star_diff: 0.3917\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 141s 6ms/step - loss: 1.2168 - acc: 0.4998 - mean_star_diff: 0.3779\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 145s 6ms/step - loss: 1.2022 - acc: 0.5048 - mean_star_diff: 0.3344\n",
      "Retrieving/Preparing data for: vocab size = 50 review_length = 500 num_reviews = 25000\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 320s 13ms/step - loss: 1.3357 - acc: 0.4535 - mean_star_diff: 0.7936\n",
      "Epoch 2/5\n",
      "21056/25000 [========================>.....] - ETA: 49s - loss: 1.2393 - acc: 0.4899 - mean_star_diff: 0.4711"
     ]
    }
   ],
   "source": [
    "vocab_sizes = [50, 100, 150, 200, 250, 500]\n",
    "review_lengths = [150, 250, 500]\n",
    "num_epochs = 5\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    for review_length in review_lengths:\n",
    "        file_name = os.path.join(DATA_ROOT, str(vocab_size) + '.' + str(review_length) + '.' + str(num_epochs) + 'epochs.hd5')\n",
    "        \n",
    "        if not os.path.isfile(file_name):\n",
    "            x_train, x_test, y_train, y_test = get_data(vocab_size=vocab_size, review_length=review_length, num_reviews=25000)\n",
    "            model = basic_lstm_model(vocab_size=vocab_size, review_length=review_length)\n",
    "            model.fit(x_train, y_train, epochs=num_epochs, batch_size=64)\n",
    "            model.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

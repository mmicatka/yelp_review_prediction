{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, RNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = 'data'\n",
    "SEED = 1000\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    pickle_name = os.path.join(DATA_ROOT, file_name + '.pkl')\n",
    "    time_start = datetime.now()\n",
    "    if os.path.isfile(pickle_name):\n",
    "        print('loading from pickle...')\n",
    "        review_data = pd.read_pickle(pickle_name)\n",
    "    else:\n",
    "        print('loading from csv...')\n",
    "        review_data = pd.read_csv(os.path.join(DATA_ROOT, file_name))\n",
    "        review_data.to_pickle(pickle_name)\n",
    "    print('Loaded in ' + str(datetime.now() - time_start) + ' seconds')\n",
    "    return review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(vocab_size, train_text=None):\n",
    "    tokenizer_file_name = os.path.join(DATA_ROOT, 'tokenizers', 'tokenizer_' + str(vocab_size) + '.pkl')\n",
    "    time_start = datetime.now()\n",
    "    if os.path.isfile(tokenizer_file_name):\n",
    "        print('Loading tokenizer...')\n",
    "        with open(tokenizer_file_name, 'rb') as file:\n",
    "            tokenizer = pickle.load(file)\n",
    "    else:\n",
    "        print('Training tokenizer...')\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        tokenizer.fit_on_texts(train_text)\n",
    "        \n",
    "        with open(tokenizer_file_name, 'wb') as file:\n",
    "            pickle.dump(tokenizer, file)\n",
    "        \n",
    "    print('Got tokenizer for vocab size: ' + str(vocab_size) + ' in ' + str(datetime.now() - time_start))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(vocab_size=100, review_length=250, num_reviews=None):\n",
    "    print('Retrieving/Preparing data for: vocab size = ' + str(vocab_size) + ' review_length = ' + str(review_length) + ' num_reviews = ' + str(num_reviews))\n",
    "    file_name = os.path.join(DATA_ROOT, 'processed_data', str(vocab_size) + '_' + str(review_length) + '_' + str(num_reviews) + '.pkl')\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as file:\n",
    "            x_train, x_test, y_train, y_test = pickle.load(file)\n",
    "    else:\n",
    "        review_data = load_data('yelp_review.csv')\n",
    "        review_data.drop(['review_id', 'user_id', 'business_id', 'date', 'useful', 'funny', 'cool'], axis=1, inplace=True)\n",
    "        \n",
    "        x = review_data['text'].as_matrix()\n",
    "        y = pd.get_dummies(review_data['stars']).as_matrix()\n",
    "        \n",
    "        # We want our tokenizer on all of the data\n",
    "        tokenizer = get_tokenizer(vocab_size)\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.3, train_size=0.7, random_state=SEED)\n",
    "        \n",
    "        # The data is truncated after the split so we get a stratified sample\n",
    "        \n",
    "        if num_reviews:\n",
    "            print('Truncating data...')\n",
    "            x_train = x_train[0:num_reviews]\n",
    "            x_test = x_test[0:num_reviews]\n",
    "            y_train = y_train[0:num_reviews]\n",
    "            y_test = y_test[0:num_reviews]\n",
    "        \n",
    "        x_train = tokenizer.texts_to_sequences(x_train)\n",
    "        x_train = pad_sequences(x_train, maxlen=review_length)\n",
    "\n",
    "        # Fit our testing data\n",
    "        x_test = tokenizer.texts_to_sequences(x_test)\n",
    "        x_test = pad_sequences(x_test, maxlen=review_length)\n",
    "        \n",
    "        with open(file_name, 'wb') as file:\n",
    "            pickle.dump([x_train, x_test, y_train, y_test], file)\n",
    "    return\n",
    "#     return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import cpu_count\n",
    "import itertools\n",
    "\n",
    "num_cores = 2\n",
    "vocab_sizes = [50, 100, 150, 200, 250, 500]\n",
    "\n",
    "Parallel(n_jobs=num_cores)(delayed(prep_data)(vocab_size=vocab_size, num_reviews=25000) for vocab_size in vocab_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Explain rational behind this\n",
    "'''\n",
    "def mean_star_diff(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.argmax(y_true) - K.argmax(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_lstm_model(embedding_vector_length=32, dropout_rate=0.2, vocab_size=500, review_length=250):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_vector_length, input_length=review_length))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', mean_star_diff])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = get_data(vocab_size=50, num_reviews=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = basic_lstm_model(vocab_size=50)\n",
    "model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('test_save.hd5', custom_objects={'mean_star_diff': mean_star_diff})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evalute(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = get_data(vocab_size=150, num_reviews=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = basic_lstm_model(vocab_size=150)\n",
    "model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n",
    "model.save(os.path.join(DATA_ROOT, 'models', '150_250_None_10_epochs.hd5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(os.path.join(DATA_ROOT, 'models', '150_250_None_10_epochs.hd5'))\n",
    "scores = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
